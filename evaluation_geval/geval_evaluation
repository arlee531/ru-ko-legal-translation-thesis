"""
G-Eval: Translation Quality Evaluation using GPT-4o
Description: Evaluates Russian-Korean legal translations for fluency and fidelity
"""

import pandas as pd
import openai
from tqdm import tqdm
import time
import json
from typing import Dict, List, Tuple
import os


# ============================================================================
# Evaluation Prompts
# ============================================================================

FLUENCY_PROMPT = """ë‹¹ì‹ ì€ ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ ë²•ë¥  ë²ˆì—­ì˜ ìœ ì°½ì„±(Fluency)ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ã€í‰ê°€ ê¸°ì¤€ã€‘
5ì  (ë§¤ìš° ìš°ìˆ˜): í•œêµ­ì–´ ë¬¸ìž¥ì´ ë§¤ìš° ìžì—°ìŠ¤ëŸ½ê³  ë²•ë¥  ë¬¸ì²´ì— ë¶€í•©. ë¬¸ë²•ì Â·ì–´íœ˜Â·ë¬¸ë§¥ì  ì˜¤ë¥˜ ì „í˜€ ì—†ìŒ.
4ì  (ìš°ìˆ˜): ì „ë°˜ì ìœ¼ë¡œ ìžì—°ìŠ¤ëŸ½ì§€ë§Œ, ì¼ë¶€ í‘œí˜„ì´ ì•½ê°„ ë¶€ìžì—°ìŠ¤ëŸ½ê±°ë‚˜ ë²•ë¥  ë¬¸ì²´ì™€ ë‹¤ì†Œ ì–´ê¸‹ë‚¨.
3ì  (ë³´í†µ): ë¬¸ìž¥ì€ ì´í•´ ê°€ëŠ¥í•˜ë‚˜ ì–´ìƒ‰í•œ í‘œí˜„ì´ë‚˜ ë¬¸ë²•ì  ë¶€ìžì—°ìŠ¤ëŸ¬ì›€ì´ ìžˆìŒ. ë²•ë¥  ë¬¸ì²´ì™€ ê±°ë¦¬ê°€ ìžˆìŒ.
2ì  (ë¯¸í¡): ë¬¸ìž¥ì´ ë¹„ë¬¸ì— ê°€ê¹ê±°ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ë²ˆì—­íˆ¬ì ìž„. ë…í•´ëŠ” ê°€ëŠ¥í•˜ë‚˜ ìžì—°ìŠ¤ëŸ½ì§€ ì•ŠìŒ.
1ì  (ë§¤ìš° ë¯¸í¡): í•œêµ­ì–´ ë¬¸ìž¥ìœ¼ë¡œ ì„±ë¦½í•˜ê¸° ì–´ë µê±°ë‚˜ ì‹¬ê°í•˜ê²Œ ë¹„ìžì—°ìŠ¤ëŸ¬ì›€. ë²•ë¥  í…ìŠ¤íŠ¸ë¡œ ê¸°ëŠ¥ ë¶ˆê°€.

ã€í‰ê°€ ì ˆì°¨ã€‘
1. ë²ˆì—­ë¬¸ì„ ì½ê³  ë¬¸ë²•, ì–´íœ˜, ìžì—°ìŠ¤ëŸ¬ì›€, ë²•ë¥  ë¬¸ì²´ ì í•©ì„±ì„ ë¶„ì„í•˜ì„¸ìš”.
2. ìœ„ì˜ 5ë‹¨ê³„ ê¸°ì¤€ì— ë”°ë¼ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.
3. ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

{{
  "analysis": "í‰ê°€ ì´ìœ ì™€ ê·¼ê±° (2-3ë¬¸ìž¥)",
  "score": ì ìˆ˜ (1-5 ì‚¬ì´ì˜ ì •ìˆ˜)
}}

ã€ì›ë¬¸(ëŸ¬ì‹œì•„ì–´)ã€‘
{source}

ã€ë²ˆì—­ë¬¸(í•œêµ­ì–´)ã€‘
{translation}

ìœ„ ë²ˆì—­ë¬¸ì˜ ìœ ì°½ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”."""


FIDELITY_PROMPT = """ë‹¹ì‹ ì€ ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ ë²•ë¥  ë²ˆì—­ì˜ ì¶©ì‹¤ì„±(Fidelity)ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ã€í‰ê°€ ê¸°ì¤€ã€‘
5ì  (ë§¤ìš° ìš°ìˆ˜): ì›ë¬¸ì˜ ì˜ë¯¸ê°€ ì™„ì „í•˜ê³  ì •í™•í•˜ê²Œ ì „ë‹¬ë¨. ë²•ë¥  ê°œë…Â·ìš©ì–´Â·ì¡°ê±´Â·ì ˆì°¨ ë“±ì´ ëˆ„ë½ ì—†ì´ ë°˜ì˜ë¨.
4ì  (ìš°ìˆ˜): ì›ë¬¸ì˜ í•µì‹¬ ì˜ë¯¸ê°€ ì¶©ì‹¤ížˆ ë°˜ì˜ë˜ì—ˆìœ¼ë‚˜, ì„¸ë¶€ì  ë‰˜ì•™ìŠ¤ë‚˜ ì¼ë¶€ ìš©ì–´ ì„ íƒì—ì„œ ì•½ê°„ì˜ ì°¨ì´ê°€ ìžˆìŒ.
3ì  (ë³´í†µ): ì›ë¬¸ì˜ ì£¼ìš” ì˜ë¯¸ëŠ” ì „ë‹¬ë˜ì—ˆìœ¼ë‚˜, ë¶€ë¶„ì  ì˜ë¯¸ ì™œê³¡ì´ ì¡´ìž¬í•¨.
2ì  (ë¯¸í¡): ì›ë¬¸ì˜ ì˜ë¯¸ê°€ ìƒë‹¹ ë¶€ë¶„ ì™œê³¡Â·ëˆ„ë½ë¨. ë²•ë¥ ì  í•µì‹¬ ìš”ì†Œ(ì¡°ê±´, ì£¼ì²´, ë²”ìœ„ ë“±)ê°€ ìž˜ëª» ì „ë‹¬ë¨.
1ì  (ë§¤ìš° ë¯¸í¡): ì›ë¬¸ ì˜ë¯¸ë¥¼ ê±°ì˜ ì „ë‹¬í•˜ì§€ ëª»í•¨. ë²•ë¥ ì  í•¨ì˜ê°€ í¬ê²Œ ì™œê³¡ë˜ì–´ ì›ë¬¸ê³¼ ë¶ˆì¼ì¹˜.

ã€í‰ê°€ ì ˆì°¨ã€‘
1. ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ë¹„êµí•˜ì—¬ ì˜ë¯¸ ì „ë‹¬ì˜ ì •í™•ì„±ì„ ë¶„ì„í•˜ì„¸ìš”.
2. ë²•ë¥  ìš©ì–´, ì¡°ê±´, ì£¼ì²´, ë²”ìœ„ ë“±ì´ ì •í™•ížˆ ì „ë‹¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
3. ìœ„ì˜ 5ë‹¨ê³„ ê¸°ì¤€ì— ë”°ë¼ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.
4. ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

{{
  "analysis": "í‰ê°€ ì´ìœ ì™€ ê·¼ê±° (2-3ë¬¸ìž¥)",
  "score": ì ìˆ˜ (1-5 ì‚¬ì´ì˜ ì •ìˆ˜)
}}

ã€ì›ë¬¸(ëŸ¬ì‹œì•„ì–´)ã€‘
{source}

ã€ë²ˆì—­ë¬¸(í•œêµ­ì–´)ã€‘
{translation}

ìœ„ ë²ˆì—­ë¬¸ì˜ ì¶©ì‹¤ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”."""


# ============================================================================
# Evaluation Functions
# ============================================================================

def evaluate_translation(
    source: str, 
    translation: str, 
    criterion: str = "fluency", 
    max_retries: int = 3
) -> Dict:
    """
    Evaluate translation using GPT-4o
    
    Args:
        source: Source text in Russian
        translation: Translation in Korean
        criterion: Either 'fluency' or 'fidelity'
        max_retries: Maximum number of retry attempts
        
    Returns:
        Dictionary with 'score' and 'analysis'
    """
    prompt = FLUENCY_PROMPT if criterion == "fluency" else FIDELITY_PROMPT
    prompt = prompt.format(source=source, translation=translation)
    
    for attempt in range(max_retries):
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Validate score
            if 'score' not in result or not (1 <= result['score'] <= 5):
                raise ValueError(f"Invalid score: {result.get('score')}")
            
            return result
            
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âš  Evaluation failed after {max_retries} attempts: {e}")
                return {"score": 0, "analysis": f"Error: {str(e)}"}
            time.sleep(2 ** attempt)  # Exponential backoff
    
    return {"score": 0, "analysis": "Evaluation failed"}


def evaluate_dataset(
    df: pd.DataFrame,
    model_columns: List[str],
    output_path: str = None,
    rate_limit_delay: float = 0.5
) -> pd.DataFrame:
    """
    Evaluate all translations in the dataset
    
    Args:
        df: DataFrame containing source and translation columns
        model_columns: List of model column names to evaluate
        output_path: Path to save results (optional)
        rate_limit_delay: Delay between API calls in seconds
        
    Returns:
        DataFrame with evaluation results
    """
    results = []
    total_evaluations = len(df) * len(model_columns) * 2  # 2 criteria
    
    print(f"\n{'='*70}")
    print(f"Starting evaluation: {len(df)} samples Ã— {len(model_columns)} models Ã— 2 criteria")
    print(f"Total evaluations: {total_evaluations}")
    print(f"{'='*70}\n")
    
    with tqdm(total=total_evaluations, desc="Evaluating") as pbar:
        for idx, row in df.iterrows():
            source = row['ru']
            
            for model_col in model_columns:
                translation = row[model_col]
                
                if pd.isna(translation) or not str(translation).strip():
                    results.append({
                        'id': row['id'],
                        'model': model_col,
                        'fluency_score': 0,
                        'fluency_analysis': 'Empty translation',
                        'fidelity_score': 0,
                        'fidelity_analysis': 'Empty translation',
                        'average_score': 0
                    })
                    pbar.update(2)
                    continue
                
                # Evaluate fluency
                fluency_result = evaluate_translation(source, translation, "fluency")
                time.sleep(rate_limit_delay)
                pbar.update(1)
                
                # Evaluate fidelity
                fidelity_result = evaluate_translation(source, translation, "fidelity")
                time.sleep(rate_limit_delay)
                pbar.update(1)
                
                # Calculate average
                avg_score = (fluency_result['score'] + fidelity_result['score']) / 2
                
                results.append({
                    'id': row['id'],
                    'model': model_col,
                    'fluency_score': fluency_result['score'],
                    'fluency_analysis': fluency_result['analysis'],
                    'fidelity_score': fidelity_result['score'],
                    'fidelity_analysis': fidelity_result['analysis'],
                    'average_score': round(avg_score, 2)
                })
    
    results_df = pd.DataFrame(results)
    
    # Save results if output path provided
    if output_path:
        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ“ Results saved to: {output_path}")
    
    return results_df


def calculate_statistics(results_df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate average scores by model
    
    Args:
        results_df: DataFrame with evaluation results
        
    Returns:
        DataFrame with statistics by model
    """
    stats = results_df.groupby('model').agg({
        'fluency_score': ['mean', 'std'],
        'fidelity_score': ['mean', 'std'],
        'average_score': ['mean', 'std']
    }).round(2)
    
    stats.columns = ['_'.join(col) for col in stats.columns]
    stats = stats.reset_index()
    
    return stats.sort_values('average_score_mean', ascending=False)


def generate_report(results_df: pd.DataFrame, stats_df: pd.DataFrame) -> str:
    """
    Generate evaluation report
    
    Args:
        results_df: DataFrame with evaluation results
        stats_df: DataFrame with statistics
        
    Returns:
        Report as string
    """
    report = f"""
{'='*70}
G-Eval Translation Quality Evaluation Report
{'='*70}

Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Total Samples: {len(results_df['id'].unique())}
Models Evaluated: {len(results_df['model'].unique())}
Total Evaluations: {len(results_df) * 2}

{'='*70}
Average Scores by Model
{'='*70}

"""
    
    for _, row in stats_df.iterrows():
        report += f"""
{row['model']}
{'-'*70}
Fluency:  {row['fluency_score_mean']:.2f} (Â±{row['fluency_score_std']:.2f})
Fidelity: {row['fidelity_score_mean']:.2f} (Â±{row['fidelity_score_std']:.2f})
Average:  {row['average_score_mean']:.2f} (Â±{row['average_score_std']:.2f})
"""
    
    report += f"""
{'='*70}
Evaluation Criteria
{'='*70}

Fluency (ìœ ì°½ì„±):
- Measures naturalness, grammar, and appropriateness for legal text
- Scale: 1 (very poor) to 5 (excellent)

Fidelity (ì¶©ì‹¤ì„±):
- Measures accuracy and completeness of meaning transfer
- Scale: 1 (very poor) to 5 (excellent)

{'='*70}
Notes:
- Evaluated using GPT-4o model via OpenAI API
- Temperature: 0 (deterministic)
- JSON output format for structured results
{'='*70}
"""
    
    return report


# ============================================================================
# Main Function
# ============================================================================

def main():
    """Main execution function"""
    
    print("="*70)
    print("G-Eval: Translation Quality Evaluation")
    print("="*70)
    
    # Setup OpenAI API
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        api_key = input("Enter your OpenAI API key: ")
    
    openai.api_key = api_key
    
    # Test API connection
    try:
        test_response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "test"}],
            max_tokens=10
        )
        print("âœ“ API connection successful")
    except Exception as e:
        print(f"âœ— API connection failed: {e}")
        return
    
    # Load data
    data_file = input("\nEnter path to CSV file: ")
    df = pd.read_csv(data_file)
    print(f"âœ“ Loaded {len(df)} samples")
    
    # Identify model columns
    model_columns = [col for col in df.columns if col not in ['id', 'ru', 'reference']]
    print(f"âœ“ Models to evaluate: {model_columns}")
    
    # Run evaluation
    output_file = "geval_results.csv"
    results_df = evaluate_dataset(df, model_columns, output_file)
    
    # Calculate statistics
    stats_df = calculate_statistics(results_df)
    print(f"\n{'='*70}")
    print("Statistics Summary")
    print(f"{'='*70}\n")
    print(stats_df.to_string(index=False))
    
    # Generate report
    report = generate_report(results_df, stats_df)
    report_file = "geval_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nâœ“ Report saved to: {report_file}")
    
    print(f"\n{'='*70}")
    print("Evaluation Complete! ðŸŽ‰")
    print(f"{'='*70}")


if __name__ == "__main__":
    main()
