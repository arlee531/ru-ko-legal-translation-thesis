"""
G-Eval: Translation Quality Evaluation using GPT-4o
Description: Evaluates Russian-Korean legal translations for fluency and fidelity
"""

import pandas as pd
import openai
from tqdm import tqdm
import time
import json
from typing import Dict, List, Tuple
import os


# ============================================================================
# Evaluation Prompts
# ============================================================================

FLUENCY_PROMPT = """You are an expert evaluator of Russian-Korean legal translation fluency.

ã€Evaluation Criteriaã€‘
5 (Excellent): The Korean sentence is highly natural and conforms to legal writing style. No grammatical, lexical, or contextual errors.
4 (Good): Generally natural, but some expressions are slightly awkward or deviate somewhat from legal writing style.
3 (Fair): The sentence is understandable but contains awkward expressions or grammatical unnaturalness. Deviates from legal writing style.
2 (Poor): The sentence is close to ungrammatical or excessively translationese. Readable but not natural.
1 (Very Poor): Difficult to function as a Korean sentence or severely unnatural. Cannot function as a legal text.

ã€Evaluation Procedureã€‘
1. Read the translation and analyze grammar, vocabulary, naturalness, and appropriateness for legal writing style.
2. Assign a score according to the 5-level criteria above.
3. Respond ONLY in the following JSON format:

{{
  "analysis": "Explanation and rationale for the evaluation (2-3 sentences)",
  "score": score (integer between 1-5)
}}

ã€Source (Russian)ã€‘
{source}

ã€Translation (Korean)ã€‘
{translation}

Please evaluate the fluency of the above translation."""


FIDELITY_PROMPT = """You are an expert evaluator of Russian-Korean legal translation fidelity.

ã€Evaluation Criteriaã€‘
5 (Excellent): The meaning of the source text is completely and accurately conveyed. Legal concepts, terminology, conditions, procedures, etc. are reflected without omission.
4 (Good): The core meaning of the source is faithfully reflected, but there are slight differences in detailed nuances or some terminology choices.
3 (Fair): The main meaning of the source is conveyed, but partial semantic distortion exists.
2 (Poor): A significant portion of the source meaning is distorted or omitted. Key legal elements (conditions, subjects, scope, etc.) are incorrectly conveyed.
1 (Very Poor): Almost fails to convey the source meaning. Legal implications are significantly distorted and inconsistent with the source.

ã€Evaluation Procedureã€‘
1. Compare the source and translation to analyze the accuracy of meaning transfer.
2. Verify whether legal terminology, conditions, subjects, scope, etc. are accurately conveyed.
3. Assign a score according to the 5-level criteria above.
4. Respond ONLY in the following JSON format:

{{
  "analysis": "Explanation and rationale for the evaluation (2-3 sentences)",
  "score": score (integer between 1-5)
}}

ã€Source (Russian)ã€‘
{source}

ã€Translation (Korean)ã€‘
{translation}

Please evaluate the fidelity of the above translation."""


# ============================================================================
# Evaluation Functions
# ============================================================================

def evaluate_translation(
    source: str, 
    translation: str, 
    criterion: str = "fluency", 
    max_retries: int = 3
) -> Dict:
    """
    Evaluate translation using GPT-4o
    
    Args:
        source: Source text in Russian
        translation: Translation in Korean
        criterion: Either 'fluency' or 'fidelity'
        max_retries: Maximum number of retry attempts
        
    Returns:
        Dictionary with 'score' and 'analysis'
    """
    prompt = FLUENCY_PROMPT if criterion == "fluency" else FIDELITY_PROMPT
    prompt = prompt.format(source=source, translation=translation)
    
    for attempt in range(max_retries):
        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Validate score
            if 'score' not in result or not (1 <= result['score'] <= 5):
                raise ValueError(f"Invalid score: {result.get('score')}")
            
            return result
            
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âš  Evaluation failed after {max_retries} attempts: {e}")
                return {"score": 0, "analysis": f"Error: {str(e)}"}
            time.sleep(2 ** attempt)  # Exponential backoff
    
    return {"score": 0, "analysis": "Evaluation failed"}


def evaluate_dataset(
    df: pd.DataFrame,
    model_columns: List[str],
    output_path: str = None,
    rate_limit_delay: float = 0.5
) -> pd.DataFrame:
    """
    Evaluate all translations in the dataset
    
    Args:
        df: DataFrame containing source and translation columns
        model_columns: List of model column names to evaluate
        output_path: Path to save results (optional)
        rate_limit_delay: Delay between API calls in seconds
        
    Returns:
        DataFrame with evaluation results
    """
    results = []
    total_evaluations = len(df) * len(model_columns) * 2  # 2 criteria
    
    print(f"\n{'='*70}")
    print(f"Starting evaluation: {len(df)} samples Ã— {len(model_columns)} models Ã— 2 criteria")
    print(f"Total evaluations: {total_evaluations}")
    print(f"{'='*70}\n")
    
    with tqdm(total=total_evaluations, desc="Evaluating") as pbar:
        for idx, row in df.iterrows():
            source = row['ru']
            
            for model_col in model_columns:
                translation = row[model_col]
                
                if pd.isna(translation) or not str(translation).strip():
                    results.append({
                        'id': row['id'],
                        'model': model_col,
                        'fluency_score': 0,
                        'fluency_analysis': 'Empty translation',
                        'fidelity_score': 0,
                        'fidelity_analysis': 'Empty translation',
                        'average_score': 0
                    })
                    pbar.update(2)
                    continue
                
                # Evaluate fluency
                fluency_result = evaluate_translation(source, translation, "fluency")
                time.sleep(rate_limit_delay)
                pbar.update(1)
                
                # Evaluate fidelity
                fidelity_result = evaluate_translation(source, translation, "fidelity")
                time.sleep(rate_limit_delay)
                pbar.update(1)
                
                # Calculate average
                avg_score = (fluency_result['score'] + fidelity_result['score']) / 2
                
                results.append({
                    'id': row['id'],
                    'model': model_col,
                    'fluency_score': fluency_result['score'],
                    'fluency_analysis': fluency_result['analysis'],
                    'fidelity_score': fidelity_result['score'],
                    'fidelity_analysis': fidelity_result['analysis'],
                    'average_score': round(avg_score, 2)
                })
    
    results_df = pd.DataFrame(results)
    
    # Save results if output path provided
    if output_path:
        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ“ Results saved to: {output_path}")
    
    return results_df


def calculate_statistics(results_df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate average scores by model
    
    Args:
        results_df: DataFrame with evaluation results
        
    Returns:
        DataFrame with statistics by model
    """
    stats = results_df.groupby('model').agg({
        'fluency_score': ['mean', 'std'],
        'fidelity_score': ['mean', 'std'],
        'average_score': ['mean', 'std']
    }).round(2)
    
    stats.columns = ['_'.join(col) for col in stats.columns]
    stats = stats.reset_index()
    
    return stats.sort_values('average_score_mean', ascending=False)


def generate_report(results_df: pd.DataFrame, stats_df: pd.DataFrame) -> str:
    """
    Generate evaluation report
    
    Args:
        results_df: DataFrame with evaluation results
        stats_df: DataFrame with statistics
        
    Returns:
        Report as string
    """
    report = f"""
{'='*70}
G-Eval Translation Quality Evaluation Report
{'='*70}

Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Total Samples: {len(results_df['id'].unique())}
Models Evaluated: {len(results_df['model'].unique())}
Total Evaluations: {len(results_df) * 2}

{'='*70}
Average Scores by Model
{'='*70}

"""
    
    for _, row in stats_df.iterrows():
        report += f"""
{row['model']}
{'-'*70}
Fluency:  {row['fluency_score_mean']:.2f} (Â±{row['fluency_score_std']:.2f})
Fidelity: {row['fidelity_score_mean']:.2f} (Â±{row['fidelity_score_std']:.2f})
Average:  {row['average_score_mean']:.2f} (Â±{row['average_score_std']:.2f})
"""
    
    report += f"""
{'='*70}
Evaluation Criteria
{'='*70}

Fluency:
- Measures naturalness, grammar, and appropriateness for legal text
- Scale: 1 (very poor) to 5 (excellent)

Fidelity:
- Measures accuracy and completeness of meaning transfer
- Scale: 1 (very poor) to 5 (excellent)

{'='*70}
Notes:
- Evaluated using GPT-4o model via OpenAI API
- Temperature: 0 (deterministic)
- JSON output format for structured results
{'='*70}
"""
    
    return report


# ============================================================================
# Main Function
# ============================================================================

def main():
    """Main execution function"""
    
    print("="*70)
    print("G-Eval: Translation Quality Evaluation")
    print("="*70)
    
    # Setup OpenAI API
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        api_key = input("Enter your OpenAI API key: ")
    
    openai.api_key = api_key
    
    # Test API connection
    try:
        test_response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "test"}],
            max_tokens=10
        )
        print("âœ“ API connection successful")
    except Exception as e:
        print(f"âœ— API connection failed: {e}")
        return
    
    # Load data
    data_file = input("\nEnter path to CSV file: ")
    df = pd.read_csv(data_file)
    print(f"âœ“ Loaded {len(df)} samples")
    
    # Identify model columns
    model_columns = [col for col in df.columns if col not in ['id', 'ru', 'reference']]
    print(f"âœ“ Models to evaluate: {model_columns}")
    
    # Run evaluation
    output_file = "geval_results.csv"
    results_df = evaluate_dataset(df, model_columns, output_file)
    
    # Calculate statistics
    stats_df = calculate_statistics(results_df)
    print(f"\n{'='*70}")
    print("Statistics Summary")
    print(f"{'='*70}\n")
    print(stats_df.to_string(index=False))
    
    # Generate report
    report = generate_report(results_df, stats_df)
    report_file = "geval_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nâœ“ Report saved to: {report_file}")
    
    print(f"\n{'='*70}")
    print("Evaluation Complete! ðŸŽ‰")
    print(f"{'='*70}")


if __name__ == "__main__":
    main()
