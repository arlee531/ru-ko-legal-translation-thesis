#!/usr/bin/env python3
"""
Legal Translation Model Training Script
ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ ë²•ë¥  ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì¤€ë¹„ëœ ë²•ë¥  ë„ë©”ì¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬
ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ ë²ˆì—­ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
"""

import os
import json
import torch
import warnings
from datetime import datetime
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    TrainingArguments, Trainer, DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
import numpy as np
import evaluate

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", category=UserWarning)

class LegalTranslationTrainer:
    """ë²•ë¥  ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path="config.json"):
        """
        ì´ˆê¸°í™”
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ Training device: {self.device}")
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = None
        self.model = None
        self.datasets = None
        self.trainer = None
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        default_config = {
            "model": {
                "name": "facebook/nllb-200-distilled-600M",
                "source_lang": "rus_Cyrl",
                "target_lang": "kor_Hang"
            },
            "data": {
                "train_path": "data/processed/train.json",
                "validation_path": "data/processed/validation.json",
                "test_path": "data/processed/test.json",
                "max_length": 128
            },
            "training": {
                "output_dir": "./models/trained/nllb-legal-ru-ko",
                "num_train_epochs": 3,
                "per_device_train_batch_size": 8,
                "gradient_accumulation_steps": 2,
                "learning_rate": 1e-5,
                "warmup_steps": 300,
                "weight_decay": 0.01,
                "logging_steps": 100,
                "save_steps": 500,
                "eval_steps": 500,
                "fp16": True,
                "early_stopping_patience": 3
            }
        }
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # ê¸°ë³¸ ì„¤ì •ê³¼ ì‚¬ìš©ì ì„¤ì • ë³‘í•©
                for key in user_config:
                    if key in default_config:
                        default_config[key].update(user_config[key])
                    else:
                        default_config[key] = user_config[key]
        
        return default_config
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        model_name = self.config["model"]["name"]
        print(f"ğŸ“¦ Loading model: {model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            src_lang=self.config["model"]["source_lang"],
            tgt_lang=self.config["model"]["target_lang"]
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        self.model.to(self.device)
        
        print(f"âœ… Model loaded successfully")
        print(f"   - Tokenizer vocab size: {len(self.tokenizer)}")
        print(f"   - Model parameters: {self.model.num_parameters():,}")
        
    def load_datasets(self):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š Loading datasets...")
        
        # ë°ì´í„° ë¡œë“œ
        datasets = {}
        for split in ["train", "validation", "test"]:
            path = self.config["data"][f"{split}_path"]
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                datasets[split] = Dataset.from_dict(data)
                print(f"   - {split}: {len(datasets[split])} examples")
            else:
                print(f"âš ï¸  Warning: {path} not found, skipping {split} set")
        
        self.datasets = DatasetDict(datasets)
        
        # í† í¬ë‚˜ì´ì§•
        print("ğŸ”¤ Tokenizing datasets...")
        self.datasets = self.datasets.map(
            self.tokenize_function,
            batched=True,
            remove_columns=self.datasets["train"].column_names
        )
        
        print("âœ… Datasets prepared successfully")
        
    def tokenize_function(self, examples):
        """í† í¬ë‚˜ì´ì§• í•¨ìˆ˜"""
        max_length = self.config["data"]["max_length"]
        
        # ì†ŒìŠ¤ ì–¸ì–´ ì„¤ì •
        self.tokenizer.src_lang = self.config["model"]["source_lang"]
        
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        model_inputs = self.tokenizer(
            examples["russian"],
            max_length=max_length,
            truncation=True,
            padding=True
        )
        
        # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§•
        self.tokenizer.tgt_lang = self.config["model"]["target_lang"]
        labels = self.tokenizer(
            examples["korean"],
            max_length=max_length,
            truncation=True,
            padding=True
        )
        
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs
    
    def compute_metrics(self, eval_pred):
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        predictions, labels = eval_pred
        
        # íŒ¨ë”© í† í° ì œê±°
        predictions = np.where(predictions != -100, predictions, self.tokenizer.pad_token_id)
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        
        # ë””ì½”ë”©
        decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # BLEU ìŠ¤ì½”ì–´ ê³„ì‚°
        bleu = evaluate.load("bleu")
        result = bleu.compute(
            predictions=decoded_preds,
            references=[[label] for label in decoded_labels]
        )
        
        return {
            "bleu": result["bleu"],
            "precisions": result["precisions"]
        }
    
    def setup_trainer(self):
        """íŠ¸ë ˆì´ë„ˆ ì„¤ì •"""
        print("ğŸ¯ Setting up trainer...")
        
        # í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=self.config["training"]["output_dir"],
            num_train_epochs=self.config["training"]["num_train_epochs"],
            per_device_train_batch_size=self.config["training"]["per_device_train_batch_size"],
            per_device_eval_batch_size=self.config["training"]["per_device_train_batch_size"],
            gradient_accumulation_steps=self.config["training"]["gradient_accumulation_steps"],
            learning_rate=self.config["training"]["learning_rate"],
            warmup_steps=self.config["training"]["warmup_steps"],
            weight_decay=self.config["training"]["weight_decay"],
            logging_steps=self.config["training"]["logging_steps"],
            save_steps=self.config["training"]["save_steps"],
            eval_steps=self.config["training"]["eval_steps"],
            evaluation_strategy="steps",
            save_strategy="steps",
            fp16=self.config["training"]["fp16"],
            report_to=[],
            load_best_model_at_end=True,
            metric_for_best_model="bleu",
            greater_is_better=True,
            save_total_limit=3,
            predict_with_generate=True,
            generation_max_length=self.config["data"]["max_length"]
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True
        )
        
        # ì½œë°± ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
        callbacks = []
        
        # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=self.config["training"]["early_stopping_patience"]
        )
        callbacks.append(early_stopping)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if MONITORING_AVAILABLE:
            print("ğŸ” Setting up performance monitoring...")
            performance_monitor = create_performance_monitor(
                log_dir="./logs",
                checkpoint_dir="./checkpoints",
                final_model_path="./nllb-legal-ru-ko-final",
                logging_steps=self.config["training"]["logging_steps"],
                save_steps=self.config["training"]["save_steps"]
            )
            callbacks.append(performance_monitor)
            print("âœ… Performance monitoring enabled")
            print("   - 200ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ì§„í–‰ ìƒí™© ê¸°ë¡")
            print("   - 1,000ìŠ¤í…ë§ˆë‹¤ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
            print("   - ì›Œë°ì—… ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ëª¨ë‹ˆí„°ë§")
            print("   - ìµœì¢… ëª¨ë¸ì„ './nllb-legal-ru-ko-final'ì— ì €ì¥")
        else:
            print("âš ï¸  Performance monitoring disabled - module not available")
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.datasets["train"],
            eval_dataset=self.datasets.get("validation"),
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=callbacks
        )
        
        print("âœ… Trainer setup complete")
        
    def train(self):
        """ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ Starting training...")
        print("-" * 60)
        
        # í›ˆë ¨ ì •ë³´ ì¶œë ¥
        total_steps = len(self.datasets["train"]) // (
            self.config["training"]["per_device_train_batch_size"] * 
            self.config["training"]["gradient_accumulation_steps"]
        ) * self.config["training"]["num_train_epochs"]
        
        print(f"ğŸ“Š Training Information:")
        print(f"   - Total examples: {len(self.datasets['train'])}")
        print(f"   - Batch size: {self.config['training']['per_device_train_batch_size']}")
        print(f"   - Gradient accumulation: {self.config['training']['gradient_accumulation_steps']}")
        print(f"   - Epochs: {self.config['training']['num_train_epochs']}")
        print(f"   - Total steps: {total_steps}")
        print(f"   - Learning rate: {self.config['training']['learning_rate']}")
        print("-" * 60)
        
        # í›ˆë ¨ ì‹œì‘
        start_time = datetime.now()
        train_result = self.trainer.train()
        end_time = datetime.now()
        
        # í›ˆë ¨ ê²°ê³¼ ì¶œë ¥
        training_time = end_time - start_time
        print(f"âœ… Training completed!")
        print(f"   - Training time: {training_time}")
        print(f"   - Final loss: {train_result.training_loss:.4f}")
        
        return train_result
    
    def evaluate_model(self):
        """ëª¨ë¸ í‰ê°€"""
        if "test" not in self.datasets:
            print("âš ï¸  No test dataset available for evaluation")
            return None
            
        print("ğŸ“Š Evaluating model on test set...")
        eval_result = self.trainer.evaluate(eval_dataset=self.datasets["test"])
        
        print(f"ğŸ“ˆ Test Results:")
        for key, value in eval_result.items():
            if isinstance(value, float):
                print(f"   - {key}: {value:.4f}")
            else:
                print(f"   - {key}: {value}")
                
        return eval_result
    
    def save_model(self, path=None):
        """ëª¨ë¸ ì €ì¥"""
        if path is None:
            path = self.config["training"]["output_dir"]
            
        print(f"ğŸ’¾ Saving model to: {path}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(path, exist_ok=True)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
        self.trainer.save_model(path)
        self.tokenizer.save_pretrained(path)
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        config_path = os.path.join(path, "training_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
            
        print("âœ… Model saved successfully")
        
    def test_translation(self, test_sentences=None):
        """ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
        if test_sentences is None:
            test_sentences = [
                "Ğ¡Ñ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ¾Ğ±ÑĞ·ÑƒÑÑ‚ÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.",
                "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ²ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² ÑĞ¸Ğ»Ñƒ Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° ĞµĞ³Ğ¾ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ.",
                "Ğ Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ°Ñ Ğ¤ĞµĞ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²Ğ¾Ğ¼.",
                "ĞĞ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ²Ğ»ĞµÑ‡ĞµÑ‚ Ğ·Ğ° ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ.",
                "Ğ¡ÑƒĞ´ĞµĞ±Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ·Ğ°ÑĞµĞ´Ğ°Ğ½Ğ¸Ğ¸."
            ]
        
        print("ğŸ” Testing translation quality...")
        print("-" * 60)
        
        for i, text in enumerate(test_sentences, 1):
            translation = self.translate_text(text)
            print(f"{i}. Russian: {text}")
            print(f"   Korean:  {translation}")
            print()
    
    def translate_text(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        # í† í¬ë‚˜ì´ì§•
        self.tokenizer.src_lang = self.config["model"]["source_lang"]
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.config["data"]["max_length"],
            truncation=True
        ).to(self.device)
        
        # ë²ˆì—­ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(
                    self.config["model"]["target_lang"]
                ),
                max_length=self.config["data"]["max_length"],
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )
        
        # ë””ì½”ë”©
        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return translation

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ Legal Translation Model Training")
    print("=" * 60)
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = LegalTranslationTrainer()
    
    try:
        # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        trainer.load_model_and_tokenizer()
        
        # 2. ë°ì´í„°ì…‹ ë¡œë“œ
        trainer.load_datasets()
        
        # 3. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer.setup_trainer()
        
        # 4. í›ˆë ¨ ì‹¤í–‰
        train_result = trainer.train()
        
        # 5. ëª¨ë¸ í‰ê°€
        eval_result = trainer.evaluate_model()
        
        # 6. ëª¨ë¸ ì €ì¥
        trainer.save_model()
        
        # 7. ë²ˆì—­ í…ŒìŠ¤íŠ¸
        trainer.test_translation()
        
        print("ğŸ‰ Training pipeline completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error during training: {str(e)}")
        raise e

if __name__ == "__main__":
    main()
