"""
Basic Vocabulary Statistics Analysis
ëŸ¬-í•œ ë²•ë¥  ë³‘ë ¬ ì½”í¼ìŠ¤ ê¸°ë³¸ ì–´íœ˜ í†µê³„ ë¶„ì„

Author: [Your Name]
Date: 2025
Description: TTR, ê³ ë¹ˆë„ ì–´íœ˜, ì–´íœ˜ ë‹¤ì–‘ì„± ë“± ê¸°ë³¸ í†µê³„ ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib í•œê¸€ ê¹¨ì§ ë°©ì§€)
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

def load_corpus(file_path):
    """
    ë³‘ë ¬ ì½”í¼ìŠ¤ CSV íŒŒì¼ ë¡œë”©
    
    Args:
        file_path (str): CSV íŒŒì¼ ê²½ë¡œ
        
    Returns:
        pd.DataFrame: ë¡œë”©ëœ ë°ì´í„°í”„ë ˆì„ (id, ru, ko ì»¬ëŸ¼)
    """
    df = pd.read_csv(file_path)
    print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df):,}ê°œ ë¬¸ì¥ìŒ")
    return df

def tokenize_text(text, language='ko'):
    """
    ì–¸ì–´ë³„ í† í°í™”
    
    Args:
        text (str): ì…ë ¥ í…ìŠ¤íŠ¸
        language (str): 'ko' ë˜ëŠ” 'ru'
        
    Returns:
        list: í† í° ë¦¬ìŠ¤íŠ¸
    """
    if language == 'ko':
        # í•œêµ­ì–´: ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
        tokens = text.split()
    elif language == 'ru':
        # ëŸ¬ì‹œì•„ì–´: ë‹¨ì–´ ê²½ê³„ ê¸°ì¤€ ë¶„ë¦¬
        tokens = re.findall(r'\b\w+\b', text)
    else:
        raise ValueError("languageëŠ” 'ko' ë˜ëŠ” 'ru'ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    return tokens

def calculate_basic_statistics(tokens):
    """
    ê¸°ë³¸ ì–´íœ˜ í†µê³„ ê³„ì‚°
    
    Args:
        tokens (list): í† í° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        dict: ê¸°ë³¸ í†µê³„ ì •ë³´
    """
    total_tokens = len(tokens)
    unique_tokens = len(set(tokens))
    type_token_ratio = unique_tokens / total_tokens if total_tokens > 0 else 0
    token_lengths = [len(token) for token in tokens]
    
    return {
        'total_tokens': total_tokens,
        'unique_tokens': unique_tokens,
        'type_token_ratio': type_token_ratio,
        'avg_token_length': np.mean(token_lengths) if token_lengths else 0,
        'token_lengths': token_lengths
    }

def analyze_frequency_distribution(tokens, top_n=100):
    """
    ë¹ˆë„ ë¶„í¬ ë¶„ì„
    
    Args:
        tokens (list): í† í° ë¦¬ìŠ¤íŠ¸
        top_n (int): ìƒìœ„ Nê°œ ì–´íœ˜
        
    Returns:
        dict: ë¹ˆë„ ë¶„ì„ ê²°ê³¼
    """
    freq_counter = Counter(tokens)
    
    # ê³ ë¹ˆë„ ì–´íœ˜
    top_words = freq_counter.most_common(top_n)
    
    # ì €ë¹ˆë„ ì–´íœ˜ (hapax legomena - 1íšŒë§Œ ë“±ì¥)
    hapax_count = sum(1 for count in freq_counter.values() if count == 1)
    hapax_ratio = hapax_count / len(freq_counter) if len(freq_counter) > 0 else 0
    
    return {
        'frequency_dist': freq_counter,
        'top_words': top_words,
        'hapax_count': hapax_count,
        'hapax_ratio': hapax_ratio,
        'vocabulary_size': len(freq_counter)
    }

def print_basic_summary(ko_stats, ru_stats, ko_freq, ru_freq, sentence_count):
    """
    ê¸°ë³¸ í†µê³„ ìš”ì•½ ì¶œë ¥
    """
    print("=" * 60)
    print("ëŸ¬-í•œ ë²•ë¥  ì½”í¼ìŠ¤ ê¸°ë³¸ ì–´íœ˜ í†µê³„ ë¶„ì„ ê²°ê³¼")
    print("=" * 60)
    
    print(f"\nğŸ“„ ì½”í¼ìŠ¤ ê·œëª¨:")
    print(f"  ì´ ë¬¸ì¥ìŒ: {sentence_count:,}ê°œ")
    
    print(f"\nğŸ”¤ ì–´íœ˜ í†µê³„:")
    print(f"  í•œêµ­ì–´:")
    print(f"    ì´ í† í°: {ko_stats['total_tokens']:,}ê°œ")
    print(f"    ê³ ìœ  ì–´íœ˜: {ko_stats['unique_tokens']:,}ê°œ")
    print(f"    TTR: {ko_stats['type_token_ratio']:.4f}")
    print(f"    í‰ê·  í† í° ê¸¸ì´: {ko_stats['avg_token_length']:.2f}ì")
    
    print(f"  ëŸ¬ì‹œì•„ì–´:")
    print(f"    ì´ í† í°: {ru_stats['total_tokens']:,}ê°œ")
    print(f"    ê³ ìœ  ì–´íœ˜: {ru_stats['unique_tokens']:,}ê°œ")
    print(f"    TTR: {ru_stats['type_token_ratio']:.4f}")
    print(f"    í‰ê·  í† í° ê¸¸ì´: {ru_stats['avg_token_length']:.2f}ì")
    
    print(f"\nğŸ” ì €ë¹ˆë„ ì–´íœ˜ (hapax legomena):")
    print(f"  í•œêµ­ì–´: {ko_freq['hapax_count']:,}ê°œ ({ko_freq['hapax_ratio']:.2%})")
    print(f"  ëŸ¬ì‹œì•„ì–´: {ru_freq['hapax_count']:,}ê°œ ({ru_freq['hapax_ratio']:.2%})")

def print_top_words(freq_analysis, language, top_n=100):
    """
    ê³ ë¹ˆë„ ì–´íœ˜ ì¶œë ¥
    
    Args:
        freq_analysis (dict): ë¹ˆë„ ë¶„ì„ ê²°ê³¼
        language (str): ì–¸ì–´ëª… (ì¶œë ¥ìš©)
        top_n (int): ì¶œë ¥í•  ìƒìœ„ ì–´íœ˜ ìˆ˜
    """
    print(f"\nğŸ” {language} ê³ ë¹ˆë„ ì–´íœ˜ Top {top_n}:")
    print("-" * 50)
    
    for i, (word, count) in enumerate(freq_analysis['top_words'][:top_n], 1):
        print(f"{i:3d}. {word:<20} ({count:,}íšŒ)")

def plot_comparison_charts(ko_stats, ru_stats):
    """
    ì–¸ì–´ë³„ ë¹„êµ ì°¨íŠ¸ ìƒì„±
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    languages = ['Korean', 'Russian']
    
    # í† í° ìˆ˜ ë¹„êµ
    token_counts = [ko_stats['total_tokens'], ru_stats['total_tokens']]
    axes[0].bar(languages, token_counts, color=['skyblue', 'lightcoral'])
    axes[0].set_title('Total Tokens')
    axes[0].set_ylabel('Count')
    for i, v in enumerate(token_counts):
        axes[0].text(i, v + max(token_counts)*0.01, f'{v:,}', ha='center')
    
    # ê³ ìœ  ì–´íœ˜ ìˆ˜ ë¹„êµ
    unique_counts = [ko_stats['unique_tokens'], ru_stats['unique_tokens']]
    axes[1].bar(languages, unique_counts, color=['skyblue', 'lightcoral'])
    axes[1].set_title('Unique Vocabulary')
    axes[1].set_ylabel('Count')
    for i, v in enumerate(unique_counts):
        axes[1].text(i, v + max(unique_counts)*0.01, f'{v:,}', ha='center')
    
    # TTR ë¹„êµ
    ttr_values = [ko_stats['type_token_ratio'], ru_stats['type_token_ratio']]
    axes[2].bar(languages, ttr_values, color=['skyblue', 'lightcoral'])
    axes[2].set_title('Type-Token Ratio')
    axes[2].set_ylabel('Ratio')
    for i, v in enumerate(ttr_values):
        axes[2].text(i, v + max(ttr_values)*0.01, f'{v:.4f}', ha='center')
    
    plt.tight_layout()
    plt.show()

def plot_token_length_distribution(ko_stats, ru_stats):
    """
    í† í° ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    """
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist(ko_stats['token_lengths'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.title('Korean Token Length Distribution')
    plt.xlabel('Token Length (characters)')
    plt.ylabel('Frequency')
    
    plt.subplot(1, 2, 2)
    plt.hist(ru_stats['token_lengths'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
    plt.title('Russian Token Length Distribution')
    plt.xlabel('Token Length (characters)')
    plt.ylabel('Frequency')
    
    plt.tight_layout()
    plt.show()

def plot_zipf_distribution(freq_analysis, language):
    """
    Zipf ë¶„í¬ ì‹œê°í™”
    """
    frequencies = list(freq_analysis['frequency_dist'].values())
    frequencies.sort(reverse=True)
    ranks = range(1, len(frequencies) + 1)
    
    plt.figure(figsize=(10, 6))
    plt.loglog(ranks, frequencies, 'o-', alpha=0.7, markersize=3)
    plt.title(f'{language} Word Frequency Distribution (Zipf\'s Law)')
    plt.xlabel('Rank (log scale)')
    plt.ylabel('Frequency (log scale)')
    plt.grid(True, alpha=0.3)
    plt.show()

def main_analysis(csv_file_path):
    """
    ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
    
    Args:
        csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
        
    Returns:
        tuple: (df, ko_stats, ru_stats, ko_freq, ru_freq)
    """
    # 1. ë°ì´í„° ë¡œë”©
    df = load_corpus(csv_file_path)
    
    # 2. í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    korean_text = ' '.join(df['ko'].astype(str))
    russian_text = ' '.join(df['ru'].astype(str))
    
    # 3. í† í°í™”
    print("\ní† í°í™” ì§„í–‰ ì¤‘...")
    ko_tokens = tokenize_text(korean_text, 'ko')
    ru_tokens = tokenize_text(russian_text, 'ru')
    
    # 4. ê¸°ë³¸ í†µê³„ ê³„ì‚°
    print("ê¸°ë³¸ í†µê³„ ê³„ì‚° ì¤‘...")
    ko_stats = calculate_basic_statistics(ko_tokens)
    ru_stats = calculate_basic_statistics(ru_tokens)
    
    # 5. ë¹ˆë„ ë¶„ì„
    print("ë¹ˆë„ ë¶„ì„ ì¤‘...")
    ko_freq = analyze_frequency_distribution(ko_tokens)
    ru_freq = analyze_frequency_distribution(ru_tokens)
    
    # 6. ê²°ê³¼ ì¶œë ¥
    print_basic_summary(ko_stats, ru_stats, ko_freq, ru_freq, len(df))
    print_top_words(ko_freq, "í•œêµ­ì–´", 100)
    print_top_words(ru_freq, "ëŸ¬ì‹œì•„ì–´", 100)
    
    # 7. ì‹œê°í™”
    print("\nì‹œê°í™” ìƒì„± ì¤‘...")
    plot_comparison_charts(ko_stats, ru_stats)
    plot_token_length_distribution(ko_stats, ru_stats)
    plot_zipf_distribution(ko_freq, "Korean")
    plot_zipf_distribution(ru_freq, "Russian")
    
    return df, ko_stats, ru_stats, ko_freq, ru_freq

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_file_path = "ru_ko_legal_corpus_10000.csv"
    
    # ë¶„ì„ ì‹¤í–‰
    df, ko_stats, ru_stats, ko_freq, ru_freq = main_analysis(csv_file_path)
    
    print("\nâœ… ê¸°ë³¸ ì–´íœ˜ í†µê³„ ë¶„ì„ ì™„ë£Œ!")
