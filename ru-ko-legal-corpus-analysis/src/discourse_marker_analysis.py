"""
ë‹´í™”í‘œì§€ 5ë²”ì£¼ ë¶„í¬ ë¶„ì„ ì½”ë“œ
ëŸ¬-í•œ ë²•ë¥  ì½”í¼ìŠ¤ì—ì„œ ë‹´í™”í‘œì§€ ë²”ì£¼ë³„ ë¶„í¬ ë¶„ì„

Author: [Ahreum Lee]
Date: 2025
Description: 5ë²”ì£¼ ë‹´í™”í‘œì§€ì˜ ì½”í¼ìŠ¤ ë‚´ ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import defaultdict

# ì˜ì–´ ë¼ë²¨ë¡œ ì‹œê°í™” ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

def build_discourse_marker_patterns():
    """5ë²”ì£¼ ë‹´í™”í‘œì§€ ì •ê·œì‹ íŒ¨í„´ êµ¬ì¶•"""
    
    patterns = {
        'ko': {
            'ì ‘ì†_í‘œì§€': [r'ë°', r'ë˜ëŠ”', r'ê·¸ë¦¬ê³ ', r'ê·¸ëŸ¬ë‚˜', r'ê·¸ëŸ°ë°', r'ë”°ë¼ì„œ', r'ë˜í•œ', r'ì•„ìš¸ëŸ¬'],
            'ì¡°ê±´ì–‘ë³´_í‘œì§€': [r'ë‹¤ë§Œ', r'ë‹¨', r'ë§Œì•½', r'ê²½ìš°', r'í•˜ì§€ë§Œ', r'ë¹„ë¡', r'í• ì§€ë¼ë„'],
            'ë²•ë¥ ì ˆì°¨_í‘œì§€': [r'ì—\s*ë”°ë¼', r'ì—\s*ì˜í•˜ì—¬', r'ì—\s*ê´€í•˜ì—¬', r'ì—\s*ëŒ€í•œ', r'ì—\s*ì˜ê±°í•˜ì—¬', r'ì—\s*ê·¼ê±°í•˜ì—¬'],
            'ê·œë²”ì–‘ìƒ_í‘œì§€': [r'í• \s*ìˆ˜\s*ìˆë‹¤', r'í•´ì•¼\s*í•œë‹¤', r'í•˜ì§€\s*ì•„ë‹ˆí•œë‹¤', r'ì •í•œë‹¤', r'ê·œì •í•œë‹¤', r'ê¸ˆì§€í•œë‹¤'],
            'ì°¸ì¡°_í‘œì§€': [r'ì œ\s*\d+\s*ì¡°', r'ì œ\s*\d+\s*í•­', r'í•´ë‹¹', r'ìƒê¸°', r'ë³¸', r'ë™', r'ê°™ì€']
        },
        'ru': {
            'ì ‘ì†_í‘œì§€': [r'Ğ¸', r'Ğ¸Ğ»Ğ¸', r'Ğ½Ğ¾', r'Ğ°\s+Ñ‚Ğ°ĞºĞ¶Ğµ', r'Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾', r'Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ', r'ĞºÑ€Ğ¾Ğ¼Ğµ\s+Ñ‚Ğ¾Ğ³Ğ¾'],
            'ì¡°ê±´ì–‘ë³´_í‘œì§€': [r'ĞµÑĞ»Ğ¸', r'Ğ·Ğ°\s+Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼', r'Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ\s+Ğ½Ğ°', r'Ñ…Ğ¾Ñ‚Ñ'],
            'ë²•ë¥ ì ˆì°¨_í‘œì§€': [r'Ğ²\s+ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸\s+Ñ', r'ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾', r'Ğ²\s+ÑĞ»ÑƒÑ‡Ğ°Ğµ', r'Ğ²\s+Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸', r'Ğ½Ğ°\s+Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸'],
            'ê·œë²”ì–‘ìƒ_í‘œì§€': [r'Ğ¼Ğ¾Ğ¶ĞµÑ‚', r'Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½', r'Ğ²Ğ¿Ñ€Ğ°Ğ²Ğµ', r'Ğ¾Ğ±ÑĞ·Ğ°Ğ½', r'Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ğ°ĞµÑ‚ÑÑ', r'ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚'],
            'ì°¸ì¡°_í‘œì§€': [r'ÑÑ‚Ğ°Ñ‚ÑŒÑ', r'Ğ¿ÑƒĞ½ĞºÑ‚', r'Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰\w+', r'ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½\w+', r'Ğ´Ğ°Ğ½Ğ½\w+', r'ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰\w+']
        }
    }
    
    return patterns

def find_markers_in_sentence(sentence, patterns, language):
    """ë¬¸ì¥ì—ì„œ ë‹´í™”í‘œì§€ ê²€ìƒ‰"""
    
    found_markers = defaultdict(list)
    
    for category, pattern_list in patterns[language].items():
        for pattern in pattern_list:
            matches = re.findall(pattern, sentence, re.IGNORECASE)
            if matches:
                found_markers[category].extend(matches)
    
    return found_markers

def analyze_discourse_marker_distribution(csv_file_path):
    """ë‹´í™”í‘œì§€ ë¶„í¬ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
    
    print("ë‹´í™”í‘œì§€ ë¶„í¬ ë¶„ì„ ì‹œì‘...")
    
    # ë°ì´í„° ë¡œë”©
    df = pd.read_csv(csv_file_path)
    print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df):,}ê°œ ë¬¸ì¥ìŒ")
    
    # íŒ¨í„´ êµ¬ì¶•
    patterns = build_discourse_marker_patterns()
    
    # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
    results = {
        'ko': defaultdict(lambda: {'sentences_with_marker': 0, 'total_occurrences': 0, 'examples': []}),
        'ru': defaultdict(lambda: {'sentences_with_marker': 0, 'total_occurrences': 0, 'examples': []})
    }
    
    total_sentences = len(df)
    
    # ê° ë¬¸ì¥ ë¶„ì„
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"ì§„í–‰ë¥ : {idx/total_sentences*100:.1f}%")
        
        # í•œêµ­ì–´ ë¶„ì„
        ko_sentence = str(row['ko'])
        ko_markers = find_markers_in_sentence(ko_sentence, patterns, 'ko')
        
        for category, markers in ko_markers.items():
            if markers:
                results['ko'][category]['sentences_with_marker'] += 1
                results['ko'][category]['total_occurrences'] += len(markers)
                if len(results['ko'][category]['examples']) < 5:
                    results['ko'][category]['examples'].append((ko_sentence, markers))
        
        # ëŸ¬ì‹œì•„ì–´ ë¶„ì„
        ru_sentence = str(row['ru'])
        ru_markers = find_markers_in_sentence(ru_sentence, patterns, 'ru')
        
        for category, markers in ru_markers.items():
            if markers:
                results['ru'][category]['sentences_with_marker'] += 1
                results['ru'][category]['total_occurrences'] += len(markers)
                if len(results['ru'][category]['examples']) < 5:
                    results['ru'][category]['examples'].append((ru_sentence, markers))
    
    # ë¹„ìœ¨ ê³„ì‚°
    for lang in ['ko', 'ru']:
        for category in results[lang]:
            results[lang][category]['sentence_ratio'] = \
                results[lang][category]['sentences_with_marker'] / total_sentences * 100
    
    return results, total_sentences

def print_results(results, total_sentences):
    """ê²°ê³¼ ì¶œë ¥"""
    
    print(f"\n{'='*80}")
    print("5ë²”ì£¼ ë‹´í™”í‘œì§€ ë¶„í¬ ë¶„ì„ ê²°ê³¼")
    print(f"{'='*80}")
    print(f"ì´ ë¶„ì„ ë¬¸ì¥: {total_sentences:,}ê°œ")
    
    for lang_name, lang_code in [('í•œêµ­ì–´', 'ko'), ('ëŸ¬ì‹œì•„ì–´', 'ru')]:
        print(f"\nğŸ” {lang_name} ë‹´í™”í‘œì§€ ë¶„í¬:")
        print("-" * 60)
        
        # ê²°ê³¼ ì •ë ¬ (ë¹„ìœ¨ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
        sorted_categories = sorted(
            results[lang_code].items(), 
            key=lambda x: x[1]['sentence_ratio'], 
            reverse=True
        )
        
        for category, data in sorted_categories:
            category_name = category.replace('_', '/').upper()
            print(f"\nğŸ“‚ {category_name}:")
            print(f"  ë¬¸ì¥ ìˆ˜: {data['sentences_with_marker']:,}ê°œ")
            print(f"  ë¬¸ì¥ ë¹„ìœ¨: {data['sentence_ratio']:.2f}%")
            print(f"  ì´ ì¶œí˜„ íšŸìˆ˜: {data['total_occurrences']:,}íšŒ")
            
            if data['examples']:
                print(f"  ì˜ˆì‹œ:")
                for i, (sentence, markers) in enumerate(data['examples'][:3], 1):
                    short_sentence = sentence[:80] + "..." if len(sentence) > 80 else sentence
                    print(f"    {i}. {short_sentence}")
                    print(f"       â†’ ë°œê²¬ëœ í‘œì§€: {', '.join(markers)}")

def create_comparison_table(results, total_sentences):
    """ì–¸ì–´ë³„ ë¹„êµ í‘œ ìƒì„±"""
    
    categories = ['ì ‘ì†_í‘œì§€', 'ì¡°ê±´ì–‘ë³´_í‘œì§€', 'ë²•ë¥ ì ˆì°¨_í‘œì§€', 'ê·œë²”ì–‘ìƒ_í‘œì§€', 'ì°¸ì¡°_í‘œì§€']
    category_names = ['ì ‘ì† í‘œì§€', 'ì¡°ê±´/ì–‘ë³´ í‘œì§€', 'ë²•ë¥ ì ˆì°¨ í‘œì§€', 'ê·œë²”ì–‘ìƒ í‘œì§€', 'ì°¸ì¡° í‘œì§€']
    
    comparison_data = []
    
    for i, cat in enumerate(categories):
        ko_data = results['ko'][cat]
        ru_data = results['ru'][cat]
        
        comparison_data.append({
            'ë²”ì£¼': category_names[i],
            'í•œêµ­ì–´_ë¬¸ì¥ìˆ˜': ko_data['sentences_with_marker'],
            'í•œêµ­ì–´_ë¹„ìœ¨': f"{ko_data['sentence_ratio']:.2f}%",
            'ëŸ¬ì‹œì•„ì–´_ë¬¸ì¥ìˆ˜': ru_data['sentences_with_marker'],
            'ëŸ¬ì‹œì•„ì–´_ë¹„ìœ¨': f"{ru_data['sentence_ratio']:.2f}%",
            'ì°¨ì´': f"{ko_data['sentence_ratio'] - ru_data['sentence_ratio']:+.2f}%"
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    
    print(f"\n{'='*100}")
    print("ë‹´í™”í‘œì§€ ë²”ì£¼ë³„ ì–¸ì–´ ê°„ ë¹„êµí‘œ")
    print(f"{'='*100}")
    print(comparison_df.to_string(index=False))
    
    return comparison_df

def create_visualization(results, total_sentences):
    """ë¶„í¬ ì‹œê°í™” (ì˜ì–´ ë¼ë²¨)"""
    
    # ë°ì´í„° ì¤€ë¹„
    categories = ['ì ‘ì†_í‘œì§€', 'ì¡°ê±´ì–‘ë³´_í‘œì§€', 'ë²•ë¥ ì ˆì°¨_í‘œì§€', 'ê·œë²”ì–‘ìƒ_í‘œì§€', 'ì°¸ì¡°_í‘œì§€']
    category_names = ['Conjunctive', 'Conditional/Concessive', 'Legal Procedural', 'Deontic Modal', 'Reference']
    
    ko_ratios = [results['ko'][cat]['sentence_ratio'] for cat in categories]
    ru_ratios = [results['ru'][cat]['sentence_ratio'] for cat in categories]
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    x = np.arange(len(categories))
    width = 0.35
    
    # ë§‰ëŒ€ ê·¸ë˜í”„
    bars1 = ax1.bar(x - width/2, ko_ratios, width, label='Korean', color='skyblue', alpha=0.8)
    bars2 = ax1.bar(x + width/2, ru_ratios, width, label='Russian', color='lightcoral', alpha=0.8)
    
    ax1.set_xlabel('Discourse Marker Categories')
    ax1.set_ylabel('Sentence Ratio (%)')
    ax1.set_title('Distribution Comparison of Discourse Marker Categories')
    ax1.set_xticks(x)
    ax1.set_xticklabels(category_names, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # íŒŒì´ ì°¨íŠ¸ (í•œêµ­ì–´)
    ko_values = [results['ko'][cat]['sentences_with_marker'] for cat in categories]
    ax2.pie(ko_values, labels=category_names, autopct='%1.1f%%', startangle=90)
    ax2.set_title('Korean Discourse Marker Distribution')
    
    plt.tight_layout()
    plt.show()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_file_path = "ru_ko_legal_corpus_10000.csv"
    
    # ë¶„ì„ ì‹¤í–‰
    results, total_sentences = analyze_discourse_marker_distribution(csv_file_path)
    
    # ê²°ê³¼ ì¶œë ¥
    print_results(results, total_sentences)
    
    # ë¹„êµí‘œ ìƒì„±
    comparison_df = create_comparison_table(results, total_sentences)
    
    # ì‹œê°í™”
    create_visualization(results, total_sentences)
    
    print("\nâœ… ë‹´í™”í‘œì§€ ë¶„í¬ ë¶„ì„ ì™„ë£Œ!")
    
    return results, comparison_df

# ì‹¤í–‰
if __name__ == "__main__":
    results, comparison_df = main()
