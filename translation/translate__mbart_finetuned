"""
mBART-50 Fine-tuned Model Translation Script
Russian to Korean Legal Domain Translation

This script translates Russian legal texts to Korean using the mBART-50 model
fine-tuned on Russian-Korean legal parallel corpus.

Author: [Your Name]
Date: 2025
License: MIT
"""

import argparse
import pandas as pd
import torch
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from tqdm.auto import tqdm
import warnings
import os
warnings.filterwarnings('ignore')


def setup_model(model_path):
    """
    Load fine-tuned mBART-50 model and tokenizer
    
    Args:
        model_path: Path to fine-tuned model (local path or HuggingFace hub)
        
    Returns:
        tuple: (model, tokenizer, device)
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    print(f"\nLoading fine-tuned model from: {model_path}")
    
    # Check if model exists locally
    if os.path.exists(model_path):
        print("Loading from local path...")
        local_files_only = True
    else:
        print("Loading from HuggingFace Hub...")
        local_files_only = False
    
    try:
        # Load tokenizer
        tokenizer = MBart50TokenizerFast.from_pretrained(
            model_path,
            src_lang="ru_RU",
            tgt_lang="ko_KR",
            local_files_only=local_files_only
        )
        print("Tokenizer loaded")
        
        # Load model
        model = MBartForConditionalGeneration.from_pretrained(
            model_path,
            local_files_only=local_files_only
        )
        model = model.to(device)
        model.eval()
        print("Model loaded successfully")
        print(f"Parameters: {model.num_parameters():,}")
        
    except Exception as e:
        print(f"Error loading model: {e}")
        print("\nTrying alternative method with trust_remote_code...")
        
        tokenizer = MBart50TokenizerFast.from_pretrained(
            model_path,
            src_lang="ru_RU",
            tgt_lang="ko_KR",
            trust_remote_code=True
        )
        model = MBartForConditionalGeneration.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        model = model.to(device)
        model.eval()
        print("Model loaded with alternative method")
    
    return model, tokenizer, device


def translate_batch(texts, model, tokenizer, device,
                   src_lang="ru_RU", tgt_lang="ko_KR",
                   batch_size=8, max_length=256, num_beams=5):
    """
    Translate texts in batches using mBART-50 fine-tuned model
    
    Args:
        texts: List of source texts in Russian
        model: Fine-tuned mBART model
        tokenizer: mBART tokenizer
        device: cuda or cpu
        src_lang: Source language code (ru_RU for Russian)
        tgt_lang: Target language code (ko_KR for Korean)
        batch_size: Number of sentences per batch
        max_length: Maximum token length
        num_beams: Beam size for beam search
        
    Returns:
        list: Translated texts in Korean
    """
    translations = []
    
    # Process in batches
    for i in tqdm(range(0, len(texts), batch_size), desc="Translating"):
        batch_texts = texts[i:i+batch_size]
        
        try:
            # Set source language
            tokenizer.src_lang = src_lang
            
            # Tokenize input
            inputs = tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(device)
            
            # Get target language token ID
            forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]
            
            # Generate translation
            with torch.no_grad():
                generated_tokens = model.generate(
                    **inputs,
                    forced_bos_token_id=forced_bos_token_id,
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True
                )
            
            # Decode translation
            batch_translations = tokenizer.batch_decode(
                generated_tokens,
                skip_special_tokens=True
            )
            
            translations.extend(batch_translations)
            
        except Exception as e:
            print(f"\nError in batch {i//batch_size}: {e}")
            # Add error markers for failed translations
            translations.extend(["[ERROR]"] * len(batch_texts))
        
        # Clear GPU cache periodically
        if device == "cuda" and (i + batch_size) % 100 == 0:
            torch.cuda.empty_cache()
    
    return translations


def translate_dataset(input_file, output_file, model, tokenizer, device, batch_size=8):
    """
    Translate entire dataset
    
    Args:
        input_file: Path to input CSV file (must contain 'ru' column)
        output_file: Path to output CSV file
        model: Fine-tuned mBART model
        tokenizer: mBART tokenizer
        device: cuda or cpu
        batch_size: Number of sentences per batch
    """
    # Load data
    print(f"\nLoading data from: {input_file}")
    df = pd.read_csv(input_file)
    print(f"Loaded: {len(df):,} sentences")
    
    if 'ru' not in df.columns:
        raise ValueError("Input CSV must contain 'ru' column with Russian text")
    
    # Estimate time
    if device == "cuda":
        print(f"Estimated time: ~{len(df) * 0.8 / 60:.1f} minutes for {len(df)} sentences")
    else:
        print(f"Estimated time: ~{len(df) * 3 / 60:.1f} minutes for {len(df)} sentences (CPU)")
    
    # Translate
    print("\nStarting translation...")
    translations = translate_batch(
        df['ru'].tolist(),
        model,
        tokenizer,
        device,
        batch_size=batch_size
    )
    
    # Add translations to dataframe
    df['mbart_finetuned'] = translations
    
    # Check for errors
    error_count = df['mbart_finetuned'].str.contains('[ERROR]', regex=False).sum()
    if error_count > 0:
        print(f"\nWarning: {error_count} translation errors detected")
    else:
        print(f"\nSuccess: All {len(df)} sentences translated")
    
    # Save results
    print(f"\nSaving results to: {output_file}")
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print("Translation completed successfully!")
    
    # Show sample results
    print("\n" + "="*70)
    print("Sample Results (first 3)")
    print("="*70)
    for i in range(min(3, len(df))):
        print(f"\n[{i+1}]")
        print(f"Source: {df['ru'].iloc[i][:80]}...")
        print(f"Translation: {df['mbart_finetuned'].iloc[i][:80]}...")
        if 'ko' in df.columns:
            print(f"Reference: {df['ko'].iloc[i][:80]}...")
    
    return df


def main():
    parser = argparse.ArgumentParser(
        description='Translate Russian legal texts to Korean using fine-tuned mBART-50 model'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Input CSV file path (must contain "ru" column)'
    )
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='Output CSV file path'
    )
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='Path to fine-tuned model (local path or HuggingFace hub)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=8,
        help='Batch size for translation (default: 8)'
    )
    
    args = parser.parse_args()
    
    print("="*70)
    print("mBART-50 Fine-tuned Model Translation")
    print("="*70)
    
    # Setup model
    model, tokenizer, device = setup_model(args.model)
    
    # Translate dataset
    translate_dataset(
        args.input,
        args.output,
        model,
        tokenizer,
        device,
        batch_size=args.batch_size
    )
    
    # Clean up
    del model
    del tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("\nGPU memory cleared")
    
    print("\n" + "="*70)
    print("COMPLETED!")
    print("="*70)


if __name__ == "__main__":
    main()
