"""
mBART-50 Original Model Translation Script
Russian to Korean Legal Domain Translation

This script translates Russian legal texts to Korean using the pretrained
facebook/mbart-large-50-many-to-many-mmt model without fine-tuning.

Author: [Your Name]
Date: 2025
License: MIT
"""

import argparse
import pandas as pd
import torch
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')


def setup_model(model_name="facebook/mbart-large-50-many-to-many-mmt"):
    """
    Load mBART-50 model and tokenizer
    
    Args:
        model_name: HuggingFace model identifier
        
    Returns:
        tuple: (model, tokenizer, device)
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    print(f"\nLoading model: {model_name}")
    tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
    model = MBartForConditionalGeneration.from_pretrained(model_name)
    model = model.to(device)
    model.eval()
    
    print(f"Model loaded successfully")
    print(f"Parameters: {model.num_parameters():,}")
    
    return model, tokenizer, device


def translate_text(text, model, tokenizer, device,
                   src_lang="ru_RU", tgt_lang="ko_KR",
                   max_length=256, num_beams=5):
    """
    Translate a single text using mBART-50 model
    
    Args:
        text: Source text in Russian
        model: mBART model
        tokenizer: mBART tokenizer
        device: cuda or cpu
        src_lang: Source language code (ru_RU for Russian)
        tgt_lang: Target language code (ko_KR for Korean)
        max_length: Maximum token length
        num_beams: Beam size for beam search
        
    Returns:
        str: Translated text in Korean
    """
    try:
        # Set source language
        tokenizer.src_lang = src_lang
        
        # Tokenize input
        encoded = tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(device)
        
        # Get target language token ID
        forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]
        
        # Generate translation
        with torch.no_grad():
            generated_tokens = model.generate(
                **encoded,
                forced_bos_token_id=forced_bos_token_id,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True
            )
        
        # Decode translation
        translation = tokenizer.batch_decode(
            generated_tokens,
            skip_special_tokens=True
        )[0]
        
        return translation
        
    except Exception as e:
        print(f"\nError during translation: {type(e).__name__} - {str(e)}")
        return "[ERROR]"


def translate_dataset(input_file, output_file, model, tokenizer, device):
    """
    Translate entire dataset
    
    Args:
        input_file: Path to input CSV file (must contain 'ru' column)
        output_file: Path to output CSV file
        model: mBART model
        tokenizer: mBART tokenizer
        device: cuda or cpu
    """
    # Load data
    print(f"\nLoading data from: {input_file}")
    df = pd.read_csv(input_file)
    print(f"Loaded: {len(df):,} sentences")
    
    if 'ru' not in df.columns:
        raise ValueError("Input CSV must contain 'ru' column with Russian text")
    
    # Estimate time
    if device == "cuda":
        print("Estimated time: 10-15 minutes for 1000 sentences")
    else:
        print("Estimated time: 1-2 hours for 1000 sentences (CPU)")
    
    # Translate
    print("\nStarting translation...")
    translations = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Translating"):
        ru_text = row['ru']
        translation = translate_text(ru_text, model, tokenizer, device)
        translations.append(translation)
        
        # Clear GPU cache periodically
        if device == "cuda" and (idx + 1) % 100 == 0:
            torch.cuda.empty_cache()
    
    # Add translations to dataframe
    df['mbart_original'] = translations
    
    # Check for errors
    error_count = df['mbart_original'].str.contains('[ERROR]', regex=False).sum()
    if error_count > 0:
        print(f"\nWarning: {error_count} translation errors detected")
    else:
        print(f"\nSuccess: All {len(df)} sentences translated")
    
    # Save results
    print(f"\nSaving results to: {output_file}")
    df.to_csv(output_file, index=False)
    print("Translation completed successfully!")
    
    return df


def main():
    parser = argparse.ArgumentParser(
        description='Translate Russian legal texts to Korean using mBART-50 original model'
    )
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Input CSV file path (must contain "ru" column)'
    )
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='Output CSV file path'
    )
    parser.add_argument(
        '--model',
        type=str,
        default='facebook/mbart-large-50-many-to-many-mmt',
        help='HuggingFace model name (default: facebook/mbart-large-50-many-to-many-mmt)'
    )
    
    args = parser.parse_args()
    
    print("="*70)
    print("mBART-50 Original Model Translation")
    print("="*70)
    
    # Setup model
    model, tokenizer, device = setup_model(args.model)
    
    # Translate dataset
    translate_dataset(args.input, args.output, model, tokenizer, device)
    
    print("\n" + "="*70)
    print("COMPLETED!")
    print("="*70)


if __name__ == "__main__":
    main()
